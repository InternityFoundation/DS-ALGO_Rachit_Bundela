# -*- coding: utf-8 -*-
"""Project R.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b9BgrdNWbBcnoBGxgn6G7WwhPqACBIAJ
"""
# Commented out IPython magic to ensure Python compatibility.


# Implementing ordinary least squares (OLS)
# importing Libraries
import io
# %matplotlib inline
from google.colab import files
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Reading Data
data = pd.read_csv('PizzaData.csv')
# print(data)
data.head()

# Collecting X and Y
X = data['X'].values
Y = data['Y'].values

# Ploting Scatter Points
plt.style.use('seaborn')
plt.scatter(X,Y,color='red')
plt.title("Pizza Franchise")
plt.xlabel("annual franchise fee")
plt.ylabel("start up cost ")
plt.show()

# Mean X and Y
mean_x = np.mean(X)
mean_y = np.mean(Y)
# print(mean_x, mean_y)

# Total number of values
n = len(X)

# Finding M and C
numer = 0
denom = 0
for i in range(n):
  numer += (X[i] - mean_x) * (Y[i] - mean_y)
  denom += (X[i] - mean_x) ** 2
m = numer / denom
c = mean_y - (m * mean_x)
print(m, c)

# Plotting Values and Regression Line
max_x = np.max(X) + 100
min_x = np.min(X) - 100
# Calculating line values x and y
x = np.linspace(min_x, max_x, 1000)
y = c + m * x

# Ploting Line
plt.plot(x, y, color='blue', label='Regression Line')

# Ploting Scatter Points
plt.style.use('seaborn')
plt.scatter(X,Y,color='red')
plt.title("Pizza Franchise OLS Method")
plt.xlabel("annual franchise fee")
plt.ylabel("start up cost ")
plt.show()

def R_sq(m, c, X, Y):
  ss_t = 0
  ss_r = 0
  for i in range(n):
    y_pred = c + m * X[i]
    ss_t += (Y[i] - mean_y) ** 2
    ss_r += (Y[i] - y_pred) ** 2
    r2 = 1 - (ss_r/ss_t)
  return(r2)
R_square_OLS = (R_sq(m, c, X, Y))
print(R_square_OLS)

# --------------------------------------------------------------------------------------------------------------------------------------------------------------

# Gradient Descent Algorithm

# Reading Data
data = pd.read_csv('PizzaData.csv')
# print(data)
data.head()

# Ploting Scatter Points
plt.style.use('seaborn')
plt.scatter(X,Y,color='red')
plt.title("Pizza Franchise")
plt.xlabel("annual franchise fee")
plt.ylabel("start up cost ")
plt.show()

dataset = pd.read_csv('PizzaData.csv')

X=dataset.iloc[:,0]
y=dataset.iloc[:,1]

u = X.mean()
std = X.std()
X = (X-u)/std

def hypothesis(x,theta):
    y_ = theta[0] + theta[1]*x
    return y_

def gradient(X,Y,theta):
    m = X.shape[0]
    grad = np.zeros((2,))
    for i in range(m):
        x = X[i]
        y_ = hypothesis(x,theta)
        y = Y[i]
        grad[0] += (y_ - y)
        grad[1] += (y_ - y)*x
    return grad/m

def error(X,Y,theta):
    m = X.shape[0]
    total_error = 0.0
    for i in range(m):
        y_ = hypothesis(X[i],theta)
        total_error += (y_ - Y[i])**2

    return (total_error/m)

def gradientDescent(X,Y,max_steps=500,learning_rate =0.1):

    theta = np.zeros((2,))
    error_list = []
    theta_list = []


    for i in range(max_steps):

        # Compute grad
        grad = gradient(X,Y,theta)
        e = error(X,Y,theta)


        #Update theta
        theta[0] = theta[0] - learning_rate*grad[0]
        theta[1] = theta[1] - learning_rate*grad[1]
        # Storing the theta values during updates
        theta_list.append((theta[0],theta[1]))
        error_list.append(e)

    return theta,error_list,theta_list

theta,error_list,theta_list = gradientDescent(X,y)

# error_list

plt.plot(error_list)
plt.title("Reduction error over time")
plt.show()

y_ = hypothesis(X,theta)
plt.scatter(X,y)
plt.plot(X,y_,color='red',label="Prediction")
plt.title("Pizza Franchise Gradient Descent Method")
plt.xlabel("annual franchise fee")
plt.ylabel("start up cost ")
plt.legend()
plt.show()

print(R_sq(theta[1], theta[0], X, Y))
